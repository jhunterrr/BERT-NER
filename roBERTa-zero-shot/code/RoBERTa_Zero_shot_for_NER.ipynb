{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot learning on roBERTa \n",
    "\n",
    "### Jamie Hunter - 40204692\n",
    "#### For use in CSC4006 - Zero shot learning for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaModel\n",
    "import torch.nn.functional as F\n",
    "from operator import itemgetter\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "def tokenize(txt):\n",
    "    segments = txt.split('@')\n",
    "    tokenized_segments = [tokenizer.tokenize(segment) for segment in segments]\n",
    "    positions = [len(s) for s in tokenized_segments]\n",
    "    positions = [positions[i]+positions[i-1] if i>0 else positions[i] for i in range(len(positions)) ]\n",
    "    \n",
    "    final_tokens = []\n",
    "    for tokens in tokenized_segments:\n",
    "        final_tokens.extend(tokens)\n",
    "    return final_tokens, positions\n",
    "\n",
    "def merge_embeddings(token_embeddings, start, end, operation = torch.mean):\n",
    "    #print('token_embedding_to merge',token_embeddings[start:end].shape, token_embeddings[start:end][:,:5])\n",
    "    merged =  operation(token_embeddings[start:end], dim = 0)\n",
    "    #print('after merging',merged.shape, merged[:5])\n",
    "    return merged\n",
    "\n",
    "# merged = merge_embeddings(token_embeddings[start:end], input[1][0],input[1][1])\n",
    "# print(merged.shape, merged[:5])\n",
    "# return merged\n",
    "\n",
    "def get_embeddings(txt, example = False):\n",
    "    input = tokenize(txt)\n",
    "    #print(input[0])\n",
    "    input_ids = torch.tensor(tokenizer.convert_tokens_to_ids(input[0])).unsqueeze(0)\n",
    "    op = model(input_ids)\n",
    "    if example:\n",
    "        return input[0] , op[0].data.squeeze(), input[1][0], input[1][1]\n",
    "    else:\n",
    "        return input[0], op[0].data.squeeze()\n",
    "    \n",
    "\n",
    "def find_entities(example, test_text, k):\n",
    "    \"\"\" Finds k-similar entites from the test_text, depending on the highlighted entity from the example\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    example: str\n",
    "        A single sentence to compare embeddings, with a highlighted entity in @@'s\n",
    "    test_text : str\n",
    "        A single sentence to compare embeddings\n",
    "    k: int\n",
    "        The number of similar entities to produce\n",
    "    \"\"\"\n",
    "    example_tokens, embeddings, start, end = get_embeddings(example, True)\n",
    "    entity_embedding = merge_embeddings(embeddings, start, end)\n",
    "\n",
    "    test_tokens , test_embeddings= get_embeddings(test_text)\n",
    "    similarity = F.cosine_similarity(test_embeddings, entity_embedding , dim = -1)\n",
    "    #print(similarity)\n",
    "    \n",
    "    #base case\n",
    "    max_similarity_index = torch.argmax(similarity)\n",
    "    \n",
    "    #get top 3 most similar instead (1 or 2 if less than 3 available)\n",
    "    if len(similarity.size()) != 0:\n",
    "        if len(similarity) > k:\n",
    "            result, max_similarity_index = torch.topk(similarity,k)\n",
    "        else: \n",
    "            result, max_similarity_index = torch.topk(similarity,len(similarity))\n",
    "            \n",
    "    \n",
    "    #print()\n",
    "    #print('Example = ',example)\n",
    "    #print('Test sentence = ', test_text)\n",
    "    if len(similarity.size()) != 0:\n",
    "        #print('Most similar entities  = to {} is {}'.format( example_tokens[start:end], itemgetter(*max_similarity_index)(test_tokens)))\n",
    "        return itemgetter(*max_similarity_index)(test_tokens)\n",
    "    else: \n",
    "        #print('Most similar entity  = to {} is {}'.format( example_tokens[start:end], test_tokens[max_similarity_index]))\n",
    "        return test_tokens[max_similarity_index]\n",
    "    \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods used for evaluation, retrieval of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_script(examples, filename, label_to_evaluate, k, merge):\n",
    "    \"\"\" Runs script that reads input file, finds a sentence example and its labels, and runs evaluate_model\n",
    "    to complete full evaluation of the corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : list\n",
    "        list of input example sentences\n",
    "    filename : str\n",
    "        The filename and location of the corpus\n",
    "    label_to_evaluate: str\n",
    "        The label in which we would like to evaluate\n",
    "    k: int\n",
    "        The number of similar entities to produce\n",
    "    merge: str\n",
    "        The merging merthod we would like to use\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialise text and value for retrieving label\n",
    "    total_entities = 0\n",
    "    found_entities = 0\n",
    "    entities_selected = 0\n",
    "    text = []\n",
    "    labels = []\n",
    "    sentence = ''\n",
    "    label_loc = 4\n",
    "    #read file\n",
    "    with open(filename) as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "                #if blank line, process method and reset sentence\n",
    "                if line.isspace() and text != []:\n",
    "                    te, fe, es = evaluate_model(examples, text, labels, label_to_evaluate, k, merge)\n",
    "                    total_entities += te\n",
    "                    found_entities += fe\n",
    "                    entities_selected += es\n",
    "                    test_text = ''\n",
    "                    text = []\n",
    "                    labels = []\n",
    "                #if not blank line, add to line, find label and assign it\n",
    "                if not line.isspace():\n",
    "                    word = line.split(' ')[0]\n",
    "                    label = line.split(' ')[label_loc-1]\n",
    "                    text.append(word)\n",
    "                    labels.append(label)\n",
    "    return total_entities, found_entities, entities_selected\n",
    "\n",
    "                \n",
    "def retrieve_person_entities(text, labels):\n",
    "    \"\"\" Returns list of the person entities from the given text and labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : list\n",
    "        The list of text tokens\n",
    "    labels : list\n",
    "        The list of labels\n",
    "    \"\"\"\n",
    "\n",
    "    person_entities = []\n",
    "    b_pos = 0\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.strip() == \"B-PER\" or label.strip() == \"I-PER\":\n",
    "            if label.strip() == \"I-PER\": \n",
    "                person_entities[b_pos] = person_entities[b_pos] + \" \" + text[i]\n",
    "            else: \n",
    "                person_entities.append(text[i])\n",
    "                b_pos = len(person_entities) - 1\n",
    "    return person_entities\n",
    "\n",
    "def retrieve_location_entities(text, labels):\n",
    "    \"\"\" Returns list of the location entities from the given text and labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : list\n",
    "        The list of text tokens\n",
    "    labels : list\n",
    "        The list of labels\n",
    "    \"\"\"\n",
    "    location_entities = []\n",
    "    b_pos = 0\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.strip() == \"B-LOC\" or label.strip() == \"I-LOC\":\n",
    "            if label.strip() == \"I-LOC\": \n",
    "                location_entities[b_pos] = location_entities[b_pos] + \" \" + text[i]\n",
    "            else: \n",
    "                location_entities.append(text[i])\n",
    "                b_pos = len(location_entities) - 1\n",
    "    return location_entities\n",
    "\n",
    "def retrieve_organisation_entities(text, labels):\n",
    "    \"\"\" Returns list of the organisation entities from the given text and labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : list\n",
    "        The list of text tokens\n",
    "    labels : list\n",
    "        The list of labels\n",
    "    \"\"\"\n",
    "    organisation_entities = []\n",
    "    b_pos = 0\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.strip() == \"B-ORG\" or label.strip() == \"I-ORG\":\n",
    "            if label.strip() == \"I-ORG\": \n",
    "                organisation_entities[b_pos] = organisation_entities[b_pos] + \" \" + text[i]\n",
    "            else: \n",
    "                organisation_entities.append(text[i])\n",
    "                b_pos = len(organisation_entities) - 1\n",
    "    return organisation_entities\n",
    "\n",
    "def retrieve_miscellaneous_entities(text, labels):\n",
    "    \"\"\" Returns list of the miscellaneous entities from the given text and labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : list\n",
    "        The list of text tokens\n",
    "    labels : list\n",
    "        The list of labels\n",
    "    \"\"\"\n",
    "    miscellaneous_entities = []\n",
    "    b_pos = 0\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.strip() == \"B-MISC\" or label.strip() == \"I-MISC\":\n",
    "            if label.strip() == \"I-MISC\": \n",
    "                miscellaneous_entities[b_pos] = miscellaneous_entities[b_pos] + \" \" + text[i]\n",
    "            else: \n",
    "                miscellaneous_entities.append(text[i])\n",
    "                b_pos = len(miscellaneous_entities) - 1\n",
    "    return miscellaneous_entities\n",
    "\n",
    "def get_average_number_entities(filename):\n",
    "    \"\"\" Returns average number of entities based on number of examples and total entities in the corpus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The filename and location of the corpus\n",
    "    \"\"\"\n",
    "    #initialise values to count all labels\n",
    "    example_count = 0\n",
    "    label_count = 0\n",
    "    label_loc = 4\n",
    "    \n",
    "    #read file\n",
    "    with open(filename) as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            #if empty line, new example, add to example_count\n",
    "            if line.isspace():\n",
    "                example_count += 1\n",
    "            #if not blank line, check for label, if not 'O' then we have entity\n",
    "            if not line.isspace() and line.split(' ')[label_loc-1].strip() != 'O':\n",
    "                label_count += 1\n",
    "    return label_count / example_count\n",
    "\n",
    "def merge_lists(entity_lists, merge):\n",
    "    similar_entities = []\n",
    "    entities_selected = 0\n",
    "    \n",
    "    # union of Value Lists\n",
    "    if merge == \"union\":\n",
    "        for entity in entity_lists:\n",
    "            similar_entities = list(set.union(set(similar_entities), entity))\n",
    "        entities_selected += len(similar_entities)\n",
    "    # intersection of value lists\n",
    "    elif merge == \"intersection\":\n",
    "        similar_entities = list(set.intersection(*map(set, entity_lists)))\n",
    "        entities_selected += len(similar_entities)\n",
    "    \n",
    "    return similar_entities, entities_selected\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(example_texts, test_text, test_labels, label_to_evaluate, k, merge):  \n",
    "    \"\"\" Runs the model retrieving k-entities, chooses a merging method and returns \n",
    "     results for use in metric evaluation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    example_texts : list\n",
    "        The list of example sentences to compare embeddings\n",
    "    test_text : str\n",
    "        A single sentence to compare embeddings\n",
    "    test_labels: list\n",
    "        A list of labels for the respective test_text\n",
    "    label_to_evaluate: str\n",
    "        The label in which we would like to evaluate\n",
    "    k: int\n",
    "        The number of similar entities to produce\n",
    "    merge: str\n",
    "        The merging merthod we would like to use\n",
    "    \"\"\"\n",
    "        \n",
    "    total_entities = 0\n",
    "    found_entities = 0\n",
    "    entities_selected = 0\n",
    "    \n",
    "    person_entities = []\n",
    "    organisation_entities = []\n",
    "    location_entities = []\n",
    "    miscellaneous_entities = []\n",
    "    \n",
    "    #finding similar entities, intersection or union\n",
    "    similar_entities = []\n",
    "    entity_lists = []\n",
    "    entities = []\n",
    "    \n",
    "    if label_to_evaluate == 'person':\n",
    "        person_entities = retrieve_person_entities(test_text, test_labels)\n",
    "    elif label_to_evaluate == 'organisation':\n",
    "        organisation_entities = retrieve_organisation_entities(test_text, test_labels)\n",
    "    elif label_to_evaluate == 'location':\n",
    "        location_entities = retrieve_location_entities(test_text, test_labels)\n",
    "    elif label_to_evaluate == 'miscellaneous':\n",
    "        miscellaneous_entities = retrieve_miscellaneous_entities(test_text, test_labels)        \n",
    "\n",
    "    text_labels = {\n",
    "        \"person\": person_entities,\n",
    "        \"organisation\": organisation_entities,\n",
    "        \"location\": location_entities,\n",
    "        \"miscellaneous\": miscellaneous_entities\n",
    "    }\n",
    "    \n",
    "    full_sentence = ''\n",
    "    for word in test_text:\n",
    "        full_sentence = full_sentence + ' ' + word\n",
    "    \n",
    "    #works for k=1 entities\n",
    "    if k == 1:\n",
    "        for example_text in example_texts:\n",
    "            entities.append(find_entities(example_text, full_sentence, k))\n",
    "            entity_lists.append(list(entities))\n",
    "            entities = []\n",
    "    else: \n",
    "        #stored list of tuples for k>1\n",
    "        for example_text in example_texts:\n",
    "            entity_lists.append(list(find_entities(example_text, full_sentence, k)))\n",
    "    \n",
    "    joined_entities, num_selected = merge_lists(entity_lists, merge)\n",
    "    \n",
    "    similar_entities = joined_entities\n",
    "    entities_selected += num_selected\n",
    "    \n",
    "    if similar_entities != None:\n",
    "        for i, entity in enumerate(similar_entities):\n",
    "            if entity[0] == 'Ä ':\n",
    "                similar_entities[i] = entity[1:]\n",
    "    \n",
    "    for entity in text_labels[label_to_evaluate]:\n",
    "        total_entities += 1\n",
    "        entity_split = entity.split()\n",
    "        for entity in entity_split:\n",
    "            if entity in similar_entities:\n",
    "                found_entities += 1\n",
    "                break\n",
    "                \n",
    "    return total_entities, found_entities, entities_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main to run evaluation of model on all values of k and different merge types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------EVALUATION, K=1, intersection------------\n",
      "Person accuracy: 10.640608034744844%\n",
      "Organisation accuracy: 6.343283582089552%\n",
      "Location accuracy: 9.744148067501362%\n",
      "Miscellaneous accuracy: 14.316702819956618%\n",
      "Accuracy: 0.09964652415418279\n",
      "Recall: 0.09964652415418279\n",
      "Precision: 0.06341045415595545\n",
      "F1 score: 0.07750212738103031\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=3, intersection------------\n",
      "Person accuracy: 27.035830618892508%\n",
      "Organisation accuracy: 19.25373134328358%\n",
      "Location accuracy: 15.351115949918345%\n",
      "Miscellaneous accuracy: 33.40563991323211%\n",
      "Accuracy: 0.22656118498569264\n",
      "Recall: 0.22656118498569264\n",
      "Precision: 0.04205855701028029\n",
      "F1 score: 0.07094665823318574\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=5, intersection------------\n",
      "Person accuracy: 34.473398479913136%\n",
      "Organisation accuracy: 28.059701492537314%\n",
      "Location accuracy: 23.897659227000545%\n",
      "Miscellaneous accuracy: 44.36008676789588%\n",
      "Accuracy: 0.31291028446389496\n",
      "Recall: 0.31291028446389496\n",
      "Precision: 0.03328975878803073\n",
      "F1 score: 0.06017739220510165\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=10, intersection------------\n",
      "Person accuracy: 41.530944625407166%\n",
      "Organisation accuracy: 41.865671641791046%\n",
      "Location accuracy: 41.2084921066957%\n",
      "Miscellaneous accuracy: 58.35140997830802%\n",
      "Accuracy: 0.441171519946137\n",
      "Recall: 0.441171519946137\n",
      "Precision: 0.024814905985495447\n",
      "F1 score: 0.04698690426037306\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=2, intersection------------\n",
      "Person accuracy: 20.955483170466884%\n",
      "Organisation accuracy: 14.17910447761194%\n",
      "Location accuracy: 11.81273816004355%\n",
      "Miscellaneous accuracy: 26.247288503253795%\n",
      "Accuracy: 0.17421309543847838\n",
      "Recall: 0.17421309543847838\n",
      "Precision: 0.050655833985904467\n",
      "F1 score: 0.0784893641223979\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=1, union------------\n",
      "Person accuracy: 21.76981541802389%\n",
      "Organisation accuracy: 18.059701492537314%\n",
      "Location accuracy: 9.744148067501362%\n",
      "Miscellaneous accuracy: 32.21258134490238%\n",
      "Accuracy: 0.18835212927116646\n",
      "Recall: 0.18835212927116646\n",
      "Precision: 0.06089464518937745\n",
      "F1 score: 0.0920343792408603\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=3, union------------\n",
      "Person accuracy: 35.28773072747014%\n",
      "Organisation accuracy: 31.268656716417908%\n",
      "Location accuracy: 15.623298856831791%\n",
      "Miscellaneous accuracy: 48.264642082429496%\n",
      "Accuracy: 0.3031476182460865\n",
      "Recall: 0.3031476182460865\n",
      "Precision: 0.03585863613738178\n",
      "F1 score: 0.06413132500089021\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=5, union------------\n",
      "Person accuracy: 39.413680781758956%\n",
      "Organisation accuracy: 37.3134328358209%\n",
      "Location accuracy: 24.115405552531303%\n",
      "Miscellaneous accuracy: 56.29067245119306%\n",
      "Accuracy: 0.3682881669752567\n",
      "Recall: 0.3682881669752567\n",
      "Precision: 0.027920984125363688\n",
      "F1 score: 0.05190676709566455\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=10, union------------\n",
      "Person accuracy: 45.27687296416938%\n",
      "Organisation accuracy: 45.74626865671642%\n",
      "Location accuracy: 41.426238432226455%\n",
      "Miscellaneous accuracy: 65.50976138828634%\n",
      "Accuracy: 0.4733209897323683\n",
      "Recall: 0.4733209897323683\n",
      "Precision: 0.02168464723890898\n",
      "F1 score: 0.041469421463227595\n",
      "----------------------------------------------------\n",
      "------------EVALUATION, K=2, union------------\n",
      "Person accuracy: 29.80456026058632%\n",
      "Organisation accuracy: 26.417910447761194%\n",
      "Location accuracy: 11.976047904191617%\n",
      "Miscellaneous accuracy: 42.624728850325376%\n",
      "Accuracy: 0.25517589631375187\n",
      "Recall: 0.25517589631375187\n",
      "Precision: 0.04334400731930466\n",
      "F1 score: 0.07410122931788743\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#text examples with highlighted entity for model to find similar word embeddings to against unseen corpus\n",
    "\n",
    "person_examples = [\"State media quoted China's top negotiator with Taipei, @Tang Shubei@, as telling a visiting group from Taiwan on Wednesday that it was time for the rivals to hold political talks.\",\n",
    "                  \"The president of the Nasdaq, @Alfred Berkeley@, was to hold a news conference Wednesday afternoon to elaborate on the new rules' effects on the market, the second largest in the world.\"]\n",
    "\n",
    "\n",
    "location_examples = [\"@Germany@'s representative to the European Union's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer.\",\n",
    "                    \"@Canada@'s largest grain handling firm said Wednesday it expects to forge a partnership with hog farmers by 1997 with a view to expanding the company's scope into pork production.\"]\n",
    "\n",
    "organisation_examples = [\"Speaking only hours after Chinese state media said the time was right to engage in political talks with Taiwan, @Foreign Ministry@ spokesman Shen Guofang told Reuters: \\\"The necessary atmosphere for the opening of the talks has been disrupted by the Taiwan authorities\\\".\",\n",
    "                         \"The Italian cabinet on Wednesday granted a reprieve for media mogul Silvio Berlusconi's @Mediaset@ television empire with a decree extending the current legal framework for television stations until Janurary 31, 1997.\"]\n",
    "\n",
    "miscellaneous_examples = [\"The pair last worked together when Scotland won the @Five Nations@ grand slam in 1990.\", \n",
    "                          \"\\\"If the (Tour's tournament) committee decides to change the rule I would not be against it, \\\"said Ballesteros, Olazabal's compatriot and @Ryder Cup@ captain.\"]\n",
    "\n",
    "#path to unseen corpus\n",
    "path = '../data/valid.txt'\n",
    "\n",
    "#get average number of entities in corpus\n",
    "average_entities = round(get_average_number_entities(path))\n",
    "\n",
    "#set for k-number similarities\n",
    "num_similar = [1,3,5,10,average_entities]\n",
    "merges = [\"intersection\", \"union\"]\n",
    "\n",
    "#for all merge types, run evaluation\n",
    "for merge in merges:\n",
    "    #for all values of k, run evaluation\n",
    "    for k in num_similar:\n",
    "    \n",
    "        #evaluation for recall and precision\n",
    "        entities_selected = 0\n",
    "        entities_relevant = 0\n",
    "        true_positives = 0\n",
    "        num_entities_chosen = 0\n",
    "\n",
    "        #evaLuation for person entities\n",
    "        total_person_entities = 0\n",
    "        found_person_entities = 0\n",
    "\n",
    "        #evaluation for organisation entities\n",
    "        total_organisation_entities = 0\n",
    "        found_organisation_entities = 0\n",
    "\n",
    "        #evaluation for location entities\n",
    "        total_location_entities = 0\n",
    "        found_location_entities = 0  \n",
    "        \n",
    "        #evaluation for miscellaneous entities\n",
    "        total_miscellaneous_entities = 0\n",
    "        found_miscellaneous_entities = 0\n",
    "        \n",
    "        #get returned results from entities found, selected and total - add to total results\n",
    "        total_entities, found_entities, num_entities_chosen = evaluate_script(person_examples, path, 'person', k, merge)\n",
    "        total_person_entities += total_entities\n",
    "        found_person_entities += found_entities\n",
    "        entities_selected += num_entities_chosen\n",
    "\n",
    "        total_entities, found_entities, num_entities_chosen = evaluate_script(location_examples, path, 'location', k, merge)\n",
    "        total_location_entities += total_entities\n",
    "        found_location_entities += found_entities\n",
    "        entities_selected += num_entities_chosen\n",
    "\n",
    "        total_entities, found_entities, num_entities_chosen = evaluate_script(organisation_examples, path, 'organisation', k, merge)\n",
    "        total_organisation_entities += total_entities\n",
    "        found_organisation_entities += found_entities\n",
    "        entities_selected += num_entities_chosen\n",
    "        \n",
    "        total_entities, found_entities, num_entities_chosen = evaluate_script(miscellaneous_examples, path, 'miscellaneous', k, merge)\n",
    "        total_miscellaneous_entities += total_entities\n",
    "        found_miscellaneous_entities += found_entities\n",
    "        entities_selected += num_entities_chosen\n",
    "\n",
    "        print(\"------------EVALUATION, K=\" + str(k) + \", \" + str(merge) + \"------------\")\n",
    "\n",
    "        if found_person_entities == 0 and total_person_entities != 0:\n",
    "            print(\"Person Accuracy: 0%\")\n",
    "        elif found_person_entities == 0 and total_person_entities == 0:\n",
    "            print(\"No Person in Examples.\")\n",
    "        else: print(\"Person accuracy: \" + str(found_person_entities/total_person_entities*100) + \"%\")\n",
    "\n",
    "        if found_organisation_entities == 0 and total_organisation_entities != 0:\n",
    "            print(\"Organisation Accuracy: 0%\")\n",
    "        elif found_organisation_entities == 0 and total_organisation_entities == 0:\n",
    "            print(\"No Organisation in Examples.\")\n",
    "        else: print(\"Organisation accuracy: \" + str(found_organisation_entities/total_organisation_entities*100) + \"%\")  \n",
    "\n",
    "        if  found_location_entities == 0 and total_location_entities != 0:\n",
    "            print(\"Location Accuracy: 0%\")\n",
    "        elif found_location_entities == 0 and total_location_entities == 0:\n",
    "            print(\"No Location in Examples.\")\n",
    "        else: print(\"Location accuracy: \" + str(found_location_entities/total_location_entities*100) + \"%\")\n",
    "            \n",
    "        if  found_miscellaneous_entities == 0 and total_miscellaneous_entities != 0:\n",
    "            print(\"Miscellaneous Accuracy: 0%\")\n",
    "        elif found_miscellaneous_entities == 0 and total_miscellaneous_entities == 0:\n",
    "            print(\"No Miscellaneous in Examples.\")\n",
    "        else: print(\"Miscellaneous accuracy: \" + str(found_miscellaneous_entities/total_miscellaneous_entities*100) + \"%\")\n",
    "\n",
    "        entities_relevant = total_location_entities + total_organisation_entities + total_person_entities + total_miscellaneous_entities\n",
    "        true_positives = found_location_entities + found_organisation_entities + found_person_entities + found_miscellaneous_entities\n",
    "        \n",
    "        \n",
    "        #accuracy and recall in this case are the same as we do not check for true negatives\n",
    "        accuracy = true_positives / entities_relevant\n",
    "        recall = true_positives / entities_relevant\n",
    "        precision = true_positives / entities_selected\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        #metric evaluation\n",
    "        print(\"Accuracy: \" + str(accuracy))\n",
    "        print(\"Recall: \" + str(recall))\n",
    "        print(\"Precision: \" + str(precision))\n",
    "        print(\"F1 score: \" + str(f1))\n",
    "\n",
    "        print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition and execution of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_entities_found (__main__.TestCalculations) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_entities_selected (__main__.TestCalculations) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_get_average_number_entities (__main__.TestCalculations) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_total_entities (__main__.TestCalculations) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_find_entities (__main__.TestFindEntities) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_max_found (__main__.TestFindEntities) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_intersection (__main__.TestMerge) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_union (__main__.TestMerge) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Nathan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_retrieve_grouped_entity (__main__.TestRetrieval) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Joe Bloggs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_retrieve_location_entities (__main__.TestRetrieval) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Belfast']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_retrieve_miscellaneous_entities (__main__.TestRetrieval) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['miscellaneous']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_retrieve_organisation_entities (__main__.TestRetrieval) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starbucks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_retrieve_person_entities (__main__.TestRetrieval) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 13 tests in 2.798s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1ca74bc2310>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestRetrieval(unittest.TestCase):\n",
    "    \n",
    "    def test_retrieve_organisation_entities(self):\n",
    "        \n",
    "        text = [\"John\", \"went\", \"to\", \"Starbucks\", \"in\", \"Belfast\"]\n",
    "        labels = [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\n",
    "        \n",
    "        organisation_entities = retrieve_organisation_entities(text, labels)\n",
    "        print(organisation_entities)\n",
    "        \n",
    "        assert organisation_entities == [\"Starbucks\"]\n",
    "    \n",
    "    def test_retrieve_person_entities(self):\n",
    "        text = [\"John\", \"went\", \"to\", \"Starbucks\", \"in\", \"Belfast\"]\n",
    "        labels = [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\n",
    "        \n",
    "        person_entities = retrieve_person_entities(text, labels)\n",
    "        print(person_entities)\n",
    "        \n",
    "        assert person_entities == [\"John\"]\n",
    "    \n",
    "    def test_retrieve_location_entities(self):\n",
    "        text = [\"John\", \"went\", \"to\", \"Starbucks\", \"in\", \"Belfast\"]\n",
    "        labels = [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\n",
    "        \n",
    "        location_entities = retrieve_location_entities(text, labels)\n",
    "        print(location_entities)\n",
    "        \n",
    "        assert location_entities == [\"Belfast\"]\n",
    "        \n",
    "    def test_retrieve_miscellaneous_entities(self):\n",
    "        text = [\"John\", \"went\", \"to\", \"test\", \"miscellaneous\"]\n",
    "        labels = [\"B-PER\", \"O\", \"O\", \"B-ORG\", \"B-MISC\"]\n",
    "        \n",
    "        miscellaneous_entities = retrieve_miscellaneous_entities(text, labels)\n",
    "        print(miscellaneous_entities)\n",
    "        \n",
    "        assert miscellaneous_entities == [\"miscellaneous\"]\n",
    "\n",
    "    def test_retrieve_grouped_entity(self):\n",
    "        text = [\"Joe\", \"Bloggs\", \"went\", \"to\", \"Starbucks\", \"in\", \"Belfast\"]\n",
    "        labels = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\n",
    "        \n",
    "        person_entities = retrieve_person_entities(text, labels)\n",
    "        print(person_entities)\n",
    "        \n",
    "        assert person_entities == [\"Joe Bloggs\"]\n",
    "\n",
    "class TestFindEntities(unittest.TestCase):\n",
    "    \n",
    "    def test_find_entities(self):\n",
    "        example_text = \"Sentence with @Entity@ tagged\"\n",
    "        test_text = \"Another sentence to find entity from\"\n",
    "        entities = find_entities(example_text, test_text, 3)\n",
    "        \n",
    "        print(len(entities))\n",
    "        \n",
    "        assert len(entities) == 3\n",
    "        \n",
    "    def test_max_found(self):\n",
    "        \n",
    "        #make sure more entities than possible are not selected\n",
    "        #should be 6 as only 6 entities to choose instead of 10\n",
    "        \n",
    "        example_text = \"Sentence with @Entity@ tagged\"\n",
    "        test_text = \"Another sentence to find entity from\" # 6 tokens\n",
    "        entities = find_entities(example_text, test_text, 10) # find k=10 entities\n",
    "        \n",
    "        assert len(entities) == len(test_text.split())\n",
    "        \n",
    "class TestMerge(unittest.TestCase):\n",
    "    \n",
    "    def test_union(self):\n",
    "        entity_lists = [[\"John\"], [\"Nathan\"]]\n",
    "        similar_entities, entities_selected = merge_lists(entity_lists, 'union')\n",
    "        \n",
    "        print(similar_entities)\n",
    "        \n",
    "        assert similar_entities == [\"John\", \"Nathan\"]\n",
    "        assert entities_selected == 2\n",
    "    \n",
    "    def test_intersection(self):\n",
    "        entity_lists = [[\"John\"], [\"John\", \"Nathan\"]]\n",
    "        similar_entities, entities_selected = merge_lists(entity_lists, 'intersection')\n",
    "        \n",
    "        print(similar_entities)\n",
    "        \n",
    "        assert similar_entities == [\"John\"]\n",
    "        assert entities_selected == 1\n",
    "        \n",
    "        \n",
    "class TestCalculations(unittest.TestCase):\n",
    "    \n",
    "    # sample data includes 2 sentences with 9 entities\n",
    "    \n",
    "    def test_get_average_number_entities(self):\n",
    "        test_file = '../data/sample_data.txt'\n",
    "        avg = get_average_number_entities(test_file)\n",
    "        # 14 entities, 2 examples\n",
    "        print(avg)\n",
    "        \n",
    "        assert avg == 4.5\n",
    "        \n",
    "    def test_total_entities(self):\n",
    "        person_examples = [\"State media quoted China's top negotiator with Taipei, @Tang Shubei@, as telling a visiting group from Taiwan on Wednesday that it was time for the rivals to hold political talks.\",\n",
    "                  \"The president of the Nasdaq, @Alfred Berkeley@, was to hold a news conference Wednesday afternoon to elaborate on the new rules' effects on the market, the second largest in the world.\"]\n",
    "        \n",
    "        test_file = '../data/sample_data.txt'\n",
    "        te, ef, es = evaluate_script(person_examples, test_file, 'person', 1, 'union')\n",
    "        \n",
    "        print(te)\n",
    "        total_entities = te\n",
    "        \n",
    "        assert total_entities == 2\n",
    "        \n",
    "    def test_entities_found(self):\n",
    "        person_examples = [\"State media quoted China's top negotiator with Taipei, @Tang Shubei@, as telling a visiting group from Taiwan on Wednesday that it was time for the rivals to hold political talks.\",\n",
    "                  \"The president of the Nasdaq, @Alfred Berkeley@, was to hold a news conference Wednesday afternoon to elaborate on the new rules' effects on the market, the second largest in the world.\"]\n",
    "        \n",
    "        test_file = '../data/sample_data.txt'\n",
    "        te, ef, es = evaluate_script(person_examples, test_file, 'person', 1, 'union')\n",
    "        \n",
    "        print(ef)\n",
    "        entities_found = ef\n",
    "        \n",
    "        ##entities_found can be confirmed by using the find_entities method\n",
    "        assert entities_found == 2\n",
    "        \n",
    "    def test_entities_selected(self):\n",
    "        person_examples = [\"State media quoted China's top negotiator with Taipei, @Tang Shubei@, as telling a visiting group from Taiwan on Wednesday that it was time for the rivals to hold political talks.\",\n",
    "                  \"The president of the Nasdaq, @Alfred Berkeley@, was to hold a news conference Wednesday afternoon to elaborate on the new rules' effects on the market, the second largest in the world.\"]\n",
    "        \n",
    "        test_file = '../data/sample_data.txt'\n",
    "        te, ef, es = evaluate_script(person_examples, test_file, 'person', 1, 'union')\n",
    "        \n",
    "        print(es)\n",
    "        entities_selected = es\n",
    "        \n",
    "        assert entities_selected == 2\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
